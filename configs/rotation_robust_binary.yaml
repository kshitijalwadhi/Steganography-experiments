# Data Configuration
data:
  root_dir: ./data
  cover_dataset: CIFAR10
  secret_type: image  # Switch back to binary secrets
  secret_dataset: MNIST # Not applicable for binary
  image_folder_cover: null
  image_folder_secret: null 
  image_size: 64
  num_workers: 4
  # secret_length is handled internally by PairedDataset for binary images (H*W)

# Model Configuration
model:
  prep_out_channels: 64  # Increased from 32 for more capacity to hide
  # HideNet input channels: 3 (cover) + prep_out_channels (64) = 67
  hide_in_channels: 67  # Updated to match new prep_out_channels
  # reveal_out_channels is set dynamically in models.py (will be 1 for binary)

# Training Configuration
training:
  batch_size: 32
  epochs: 6 # Adjust as needed
  lr: 1e-4
  lam_hide: 10.0  # Increased from 1.2 to prioritize visual quality of stego
  lam_reveal: 0.8  # Decreased from 1.0 to emphasize image hiding over reveal accuracy
  checkpoint_dir: ./outputs/checkpoints/rotation-robust-mnist-alltransf
  checkpoint_freq: 5  # Save every N epochs
  val_freq: 1  # Validate every N epochs
  device: auto  # Options: auto, cuda, cpu
  wandb:
    enabled: true
    project: "deep-steganography-rotation"
    entity: null # Your wandb username or team name
    run_name: "rotation-robust-mnist-alltransf-inv-v1"

# Rotation Training Configuration
rotation:
  enabled: true
  # Probability of applying each rotation during training
  probs: [0.25, 0.25, 0.25, 0.25]  # [0째, 90째, 180째, 270째]
  rotation_angles: [0, 90, 180, 270]
  prob: 0.5 # Probability of applying rotation transformation

# Translation Training Configuration
translation:
  enabled: true
  # Max translation shift as a fraction of image size (e.g., 0.1 means +/- 10% shift)
  max_shift_fraction: 0.125 # E.g. +/- 8 pixels for 64x64
  # Probability of applying translation during training (independent of others)
  prob: 0.5

# Scaling Training Configuration
scaling:
  enabled: true
  # Min/Max scaling factor (1.0 means no scaling)
  min_scale_factor: 0.5
  max_scale_factor: 1.1
  # Probability of applying scaling during training (independent of others)
  prob: 0.5

# Evaluation Configuration
evaluation:
  batch_size: 32
  # Metrics: psnr/ssim compare cover and stego
  # secret_acc_aligned will be calculated in the test script
  test_angles: [0, 90, 180, 270]
  test_scales: [1.0, 0.9, 1.1, 0.8, 1.2]
  test_translations_fraction: [0.0, 0.125, 0.25] # Test no shift and +/- 8 pixels (for 64x64) # Example translation fractions (0.0 = identity)
  metrics: [psnr, ssim] 

# Embedding/Extraction Configuration
embed:
  cover_image: null
  secret_image: null # Binary secrets generated on the fly
  output_image: ./outputs/stego_rotation_robust_invisible.png
  # Checkpoint path used by test_rotation and experiment_rotation modes
  checkpoint_path: ./outputs/checkpoints/rotation-robust-mnist-alltransf/best_checkpoint.pth 

extract:
  stego_image: null
  output_secret: ./outputs/revealed_secret_rotation_robust_invisible.png
  checkpoint_path: ./outputs/checkpoints/rotation-robust-mnist-alltransf/best_checkpoint.pth 